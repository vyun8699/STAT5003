---
title: "STAT5003_14_Credit_Card_v1"
author: "STAT5003 group project 14"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
#libraries
library(readr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(scales)
library(GGally)
library(caret)
library(smotefamily)
library(randomForest)
library(reshape2)
library(MLmetrics)
library(ggrepel)
```

### Problem overview

The goal of this exercise is to define 'bad debtors' based on their credit history and identify if a pattern or signal exists amongst credit applicants which may suggest the tendency of a certain cohort to become bad debtors. Management of bad debt is a key value driver in banking / the credit industry. Insights from this analysis may assist banks in acquiring the right customers (e.g. spend marketing budget on potential customers who are less likely to default) and decide on their credit risk management strategy to minimize loss (e.g. allow for different starting debt ceiling for different customer classes, sell off riskier customer portfolio and retain safer ones, etc). The dataset can be found [here](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction).
 
### Dataset description

```{r, message=FALSE, warning=FALSE, results='hold'}
# load dataset(s)
# source: https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction

#get and set path to current working directory
path = getwd()
setwd(path)

#read csv
application_record <- read.csv('application_record.csv', header = TRUE)
credit_record <- read.csv('credit_record.csv', header = TRUE)

# rename features for code simplicity
application_record <- application_record %>%
  rename (gender = CODE_GENDER,
          car_flag = FLAG_OWN_CAR,
          realty_flag = FLAG_OWN_REALTY,
          children_count= CNT_CHILDREN,
          income_total = AMT_INCOME_TOTAL,
          income_type = NAME_INCOME_TYPE,
          education_type = NAME_EDUCATION_TYPE,
          family_status = NAME_FAMILY_STATUS,
          housing_type = NAME_HOUSING_TYPE,
          days_age = DAYS_BIRTH,
          days_employed = DAYS_EMPLOYED,
          mobilephone_flag=FLAG_MOBIL,
          workphone_flag = FLAG_WORK_PHONE,
          phone_flag = FLAG_PHONE,
          email_flag = FLAG_EMAIL,
          occupation_type = OCCUPATION_TYPE,
          familymembers_count = CNT_FAM_MEMBERS
          )

credit_record <- credit_record %>%
  rename (months_balance = MONTHS_BALANCE,
          credit_status = STATUS
          )

#print dimensions
cat("Dimensions of application_record: Rows =", dim(application_record)[1], "Columns =", dim(application_record)[2], "\n")
cat("Dimensions of credit_record: Rows =", dim(credit_record)[1], "Columns =", dim(credit_record)[2], "\n")
```
The dataset is split into two files (**credit_record** and **application_record**) which are connected by the feature **ID**: 

+ **application_record (18 features x 438k observations):**<br> 
  This file contains collected information at the initial credit card application, such as ID, gender, occupation, number of children, marital status, etc.

+ **credit_record (3 features x 1+ million observations):** <br>
  This file contains 46 thousand unique customers IDs which tracks their credit status overtime. Customers are classified into different loan status groups (e.g. 'paid off that month', '1-29 days past due', etc.).

Key challenges that needs to be solved to analyze this dataset include: 

+ **Large and complex dataset**:<br>
  The dataset contains more than 10,000 samples, a mix of numerical and categorical values, and will need to be combined for analysis. **credit_record** will need to be 'flattened' and matched with customer IDs that exists in  **application_record**. The resulting dataset must then be screened for customers with ample transaction history (e.g. we can only learn of 'bad debt' behavior when customer has existed for some time). Post pruning, we are left with a combined dataset of 20 features and 20k+ observations.
  
+ **Class imbalance**:<br>
  The data is imbalanced. The number of 'bad_debtors' is small compared to the overall sample size and will need to be processed to remove bias. Data imbalance is taken into account when defining target classes (e.g. if class size is too small then predicted output will be spurious) and choosing analysis techniques (e.g. choice of performance measure, down-sampling, up-sampling, SMOTE).

### Initial data analysis and cleaning

**1. Screening the dataset for nulls, blanks, and duplicates:**<br>
No nulls were found. Cells in **occupation_type** were blank (" ") for income_type = "pensioners" and were filled in accordingly. Remaining blank cells were filled with "unknown". Rows with duplicated ID numbers in application_data were dropped.

```{r, warning=FALSE, results='hold'}
# check and remove NAs --> no NAs found
na_rows_application <- application_record[!complete.cases(application_record), ]
na_rows_credit <- credit_record[!complete.cases(credit_record), ]
nrow_app <- nrow(na_rows_application) # no rows with NA
nrow_cred <- nrow(na_rows_credit) # no rows with NA

cat('Number of rows with null values in application_record:',nrow_app,'rows','\n')
cat('Number of rows with null values in credit_record:',nrow_cred,'rows','\n')

#OCCUPATION_TYPE has blank values (''), fill with 'Pensioner' and 'Unknown' accordingly

pensioner_count <- 0
unknown_count <- 0

for (i in 1:length(application_record$occupation_type)) {
  if (application_record$occupation_type[i] == "" && application_record$income_type[i] == "Pensioner") {
    application_record$occupation_type[i] <- "Pensioner"
    pensioner_count <- pensioner_count + 1
  }
}

for (i in 1:length(application_record$occupation_type)) {
  if (application_record$occupation_type[i] == "" ) {
    application_record$occupation_type[i] <- "Unknown"
    unknown_count <- unknown_count + 1
  }
}

cat("Number of blank occupation_type filled with 'Pensioner' (inferred from 'income_type'):", pensioner_count,"cells", "\n")
cat("Number of blank occupation_type filled with 'Unknown':", unknown_count,"cells", "\n")

# check and remove duplicates in application_record 
duplicate_rows <- application_record %>% group_by(ID) %>%filter(n() > 1)  
nrow_dup <- nrow(duplicate_rows) 
application_record <- application_record %>% anti_join(duplicate_rows, by = "ID")

cat("Number of duplicate IDs dropped:", nrow_dup,"rows", "\n")
```
**2. Trimming the dataset to only include relevant IDs:**<br>
Only accounts with 20+ month of transaction history were included in the final dataset to allow enough time to pass for each customers to prove their long-term credit quality.

```{r, results='hold'}
# Trim both dataset by common IDs
common_ids <- intersect(application_record$ID, credit_record$ID)
filtered_application_records <- application_record %>% filter(ID %in% common_ids) #36k x 18 fetures
filtered_credit_records <- credit_record %>% filter(ID %in% common_ids) #777k x 3 features

# We can only judge credit quality if customer has already banked with us for some time
# setting a cut off for customers that has banked with us for more than 'threshold' months

threshold <- -20 #this is a negative number since 0 = now

filtered_credit_records_months <- filtered_credit_records  %>% filter(months_balance <= threshold)
length_ids <- unique(filtered_credit_records_months$ID)

cat("There are", length(length_ids),"unique IDs with more than",-threshold,"months of transaction history.","\n")

# trim both datasets to to only include those that meets the threshold history
length_filtered_application_records <- filtered_application_records %>% 
                                          filter(ID %in% length_ids)
length_filtered_credit_records <- filtered_credit_records %>% 
                                          filter(ID %in% length_ids) 

#dimension of filtered records

cat("Dimension of trimmed application_record:", dim(length_filtered_application_records)[1], "rows x",dim(length_filtered_application_records)[2],"features.","\n")

cat("Dimension of trimmed credit_record:", dim(length_filtered_credit_records)[1], "rows x",dim(length_filtered_credit_records)[2],"features.","\n")
```

**3. Understanding the distribution of classes:** <br>
The charts below show the delinquency rate for each bad debt category - inclusive of categories after it (e.g., those with worse credit performance). For example, debtors classified as ‘0-29 days past due’ is inclusive of other debtors with 30+ days past due all the way to write offs. From this chart, we decided to focus on 30+ days delinquency as our definition of ‘bad debt’ since ‘0-29 days’ is too common and could very well be an honest mistake by the customer instead of inability to service debt.

```{r, warning = FALSE, results='hold'}
# currently, STATUS is filled with the following values:
    # X : No loan for the month
    # C : paid off for the month
    # 0 : 0-29 days past due
    # 1 : 30-59 days past due
    # 2 : 60-89 days past due
    # 3 : 90-119 days past due
    # 4 : 120-149 days past due
    # 5 : bad debt / write off

# we want to see the proportion of customers that falls into the '0' to '5' categories
filters <- list(
  c('0','1','2','3','4','5'),
  c('1','2','3','4','5'),
  c('2','3','4','5'),
  c('3','4','5'),
  c('4','5'),
  c('5')
)

category_counts <- numeric(length(filters))
category_ids <- vector("list", length(filters))

for (i in seq_along(filters)) {
  filtered_records <- length_filtered_credit_records %>% filter(credit_status %in% filters[[i]])
  category_ids[[i]] <- unique(filtered_records$ID)
  category_counts[i] <- length(category_ids[[i]])
}

df <- data.frame(
  debt_category = c('0: 0-29 days past due',
                    '1: 30-59 days past due',
                    '2: 60-89 days past due',
                    '3: 90-119 days past due',
                    '4: 120-149 days past due',
                    '5: bad debt / write off'),
  category_count = category_counts
)

df$proportion <- df$category_count / length(length_ids)

# print(df) #unhash to run

bar_plot <- ggplot(df, aes(x = debt_category)) +
  geom_bar(aes(y = category_count), stat = 'identity', fill = 'blue', alpha = 0.5) +
  labs(title = 'Bad debtors (count)', x = 'Category', y = 'Value') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
line_plot <- ggplot(df, aes(x = debt_category)) +
  geom_line(aes(y = proportion, group = 1), color = 'red', size = 1) +
  geom_text(aes(y = proportion, label = percent(proportion, accuracy = 0.1)), vjust = -0.5, color = 'red') +
  labs(title = 'Bad debtors (% total)', x = 'Category', y = 'Value') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(bar_plot, line_plot, ncol = 2)

```

**4. Extracting 'relationship_length' as an additional feature and  'debt_quality' as the target:** <br>

The team identified and added in the length of customer relationship as a potentialy useful feature. The team also created ‘debt_quality’ qualifiers and appended it to **application_record**. Value of ‘0’ for non-problematic debtors, ‘1’ for debtors with 30+ days past due. The team opted to define classes this way as higher classes (e.g. those with longer past dues) have very low counts which can further exacerbate the data imbalance or produce spurious relationships. 

```{r, warning = FALSE, results='hold'}
# start a new copy of the trimmed application dataset 
application_clean <- length_filtered_application_records

# add relationship_length into dataset
relationship_length_values <- credit_record %>% group_by(ID) %>%
  summarize(relationship_length = -min(months_balance) +1)

application_clean <- application_clean %>% left_join(relationship_length_values, by = "ID")

# add column debt_quality
application_clean$debt_quality <- 0

# depending on how hard we want to make this, we can choose how many flags we want, below we choose 2:
application_clean$debt_quality[application_clean$ID %in% category_ids[[2]]] <- 1
#application_clean$debt_quality[application_clean$ID %in% category_ids[[3]]] <- 2
#application_clean$debt_quality[application_clean$ID %in% category_ids[[4]]] <- 3
#application_clean$debt_quality[application_clean$ID %in% category_ids[[5]]] <- 4
#application_clean$debt_quality[application_clean$ID %in% category_ids[[6]]] <- 5

# Create a table with 'debt_quality' and 'count'
debt_quality_counts <- table(application_clean$debt_quality)
debt_quality_table <- data.frame(debt_quality = names(debt_quality_counts), count = as.vector(debt_quality_counts))

# Print the table
print(debt_quality_table)
```

**5. Data visualization:** <br>

The charts below show composition of bad debt (by count) and income level of each occupation type. We note that some occupancy types carry higher bad debt rates and outliers.
  
```{r, results ='hold', warning = FALSE, fig.show='hold'}
# count debt_quality > 0 for the different occupation_type
debt_count <- application_clean %>% filter(debt_quality >0) %>% group_by(occupation_type) %>% summarise(debt_count=n())
occupation_count <- application_clean %>%  group_by(occupation_type) %>% summarise(occupation_count=n()) 

result <- left_join(occupation_count, debt_count, by = "occupation_type") %>%
  mutate(ratio = debt_count / occupation_count) 

# print(result)

result$occupation_type <- factor(result$occupation_type, levels = result$occupation_type)

#bar_plot <- ggplot(result, aes(x = occupation_type)) +
#  geom_bar(aes(y = occupation_count), stat = 'identity', fill = 'blue', position = position_dodge(width = 0.8), alpha = 0.5) #+ geom_bar(aes(y = debt_count), stat = 'identity', fill = 'red', position = position_dodge(width = 0.8), alpha = 0.5) +
#  labs(title = 'Count of debtors by occupation', x = 'Category', y = 'Value') +
#  theme(axis.text.x = element_text(angle = 45, hjust = 1))

line_plot <- ggplot(result, aes(x = occupation_type)) +
  geom_line(aes(y = ratio, group = 1), color = 'red', size = 1) +
  geom_text_repel(aes(y = ratio, label = percent(ratio, accuracy = 0.1)), 
                  nudge_x = 0.0, nudge_y = 0.05, color = 'red') +
  labs(title = 'Bad debtors by occupation (% total count)', x = 'Category', y = 'Value') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(line_plot)

#grid.arrange(bar_plot, line_plot, ncol = 2)

# create scatterplot for NUMERIC(x,y) by CATEGORY(z)
#x_value <- 'employment_years'
#y_value <- 'income_total_log'
#z_value <- 'debt_quality'

#ggplot(application_clean, aes(x=.data[[x_value]], y=.data[[y_value]], color=.data[[z_value]])) + 
#  geom_point() +
#  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# create boxplot for NUMERIC by CATEGORY
x_value <-'occupation_type'
y_value <- 'income_total'

ggplot(application_clean, aes(x=.data[[x_value]], y=.data[[y_value]])) + 
  geom_boxplot() +
  labs(title = 'Boxplot of Income by Occupation Type') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Feature Engineering:

+ 'familymembers_count' was dropped due to high correlation with 'children_count'. 
+ 'income_total' transformed into 'income_total_log' to address long tail. 
+ 'mobilephone_flag' was dropped as it only had a single value. 
+ 'days_age' and 'days_employed' transformed into 'age_years' and 'employment_years'. 
+ 'children_count' was cleaned for outliers (2 rows removed, negligible impact). 

```{r, warning = FALSE, results='hold', fig.show='hold'}
# Plot 1: correlation before cleaning
application_data_numerical <- application_clean %>% select_if(is.numeric)
application_data_numerical$ID <- NULL

correlation_matrix <- cor(application_data_numerical)

corr_1_plot <- ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = 'Correlation before clean-up')+
  coord_fixed(ratio = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#add column income_total_log due to long tail of income_total
application_clean$income_total_log <- log(application_clean$income_total)

# Plot 3 and 4: income_total and income_total_log
bin_count <- 20

p_income_total <-ggplot(application_clean, aes(x = income_total)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'income_total') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

p_income_total_log<-ggplot(application_clean, aes(x = income_total_log)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'income_total_log') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

#transform days_age into age_years which is now a positive integer
application_clean$age_years = as.integer(round(-application_clean$days_age / 365))

#transform days_employed into employment_years which is now a positive integer, set minimum to 0
application_clean$employment_years = as.integer(round(-application_clean$days_employed / 365))
application_clean$employment_years[application_clean$employment_years < 0] <- 0 

# Plot 5 and 6: days_employed and employment_years
p_days_employed <-ggplot(application_clean, aes(x = days_employed)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'days employed (countdown to application date)') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

p_employment_years<-ggplot(application_clean, aes(x = employment_years)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'years employed') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))


#drop familymembers_count as it highly correlates with children_count + spouse (spouse if married)
application_clean$familymembers_count <- NULL

#drop income_total
application_clean$income_total <- NULL

#mobilephone_flag only has 1 value = 1, drop
application_clean$mobilephone_flag <- NULL

# drop days_age
application_clean$days_age <- NULL

# drop days_employed
application_clean$days_employed <- NULL

# drop ID
application_clean$ID <- NULL

# Plot 7 and 8: children_count before and after outliers removal
p_children_1 <- ggplot(application_data_numerical, aes(x = children_count)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'Children_count (with outliers)')

# remove outliers in children_count
application_clean <- subset(application_clean, children_count <= 10) # 2 rows removed
application_data_numerical <- application_clean %>% select_if(is.numeric)
application_data_numerical$ID <- NULL

p_children_2 <- ggplot(application_data_numerical, aes(x = children_count)) +
        geom_histogram(bins = bin_count) +
        labs(title = 'Children_count (outliers removed)')



#Plot 2: correlation after cleaning
correlation_matrix <- cor(application_data_numerical)

corr_2_plot <- ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = 'Correlation after clean-up')+
  coord_fixed(ratio = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#feature engineering plots
grid.arrange(p_income_total,
             p_income_total_log,
             p_days_employed,
             p_employment_years,
             p_children_1,
             p_children_2, 
             ncol = 2)
```

**Correlation between features improved post clean up, see below:**

```{r}
#layout_matrix <- rbind(c(1, 2))
#grid.arrange(corr_1_plot, 
#             corr_2_plot,
#             layout_matrix = layout_matrix)

print(corr_1_plot)
print(corr_2_plot)
```

### Pre-processing

**Pre-processing used for this analysis are described below:**<br>

+ **Min-max scaling:** applied to numerical data (excluding the target) to remove bias from large values. 
+ **One-hot-encode:** applied to categorical data (excluding the target) to create binary representation of each categorical value. This is done as some machine learning models require numerical input. 
+ **Synthetic Minority Over-sampling Technique (SMOTE):** applied to prevent bias towards majority class due to data imbalance. SMOTE works by creating synthetic data points between randomly-chosen k-nearest neighbor at random distances in the feature space. In practice, SMOTE should only be used on <mark> training data</mark> to prevent leakage. 

```{r}
# PRE PROCESSING FUNCTIONS

# FUNCTION: minmax scaler, to be used on numerical data in the below function

minmax_scale <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# FUNCTION: apply minmax to numerical data and one hot encode to categorical data.

preprocess_data <- function(dataframe, target_col) {
  #split dataframe into 3 parts for processing
  target_data <- dataframe %>% select({{ target_col }})
  dataframe <- dataframe %>% select(-{{ target_col }}) #take target_data out so it's not accidentally picked
  numerical_data <- dataframe %>% select_if(is.numeric)
  categorical_data <- dataframe %>% select_if(is.character)
  
  #minmax on numerical_data
  numerical_cols <- names(numerical_data)
  scaled_numerical_data <- numerical_data
  
  for (col_name in numerical_cols) {
    scaled_col <- minmax_scale(numerical_data[[col_name]])
    scaled_numerical_data[[col_name]] <- scaled_col
  }
  
  #one hot encode on categorical_data
  formula <- dummyVars(~ ., data = categorical_data)
  encoded_data <- predict(formula, newdata = categorical_data)
  
  #transform target_col as.factor, needed for random forest
  target_data[[target_col]] <- as.factor(target_data[[target_col]])
  
  #bind the three dataframes together
  merged_data <- cbind(scaled_numerical_data, encoded_data, target_data)
  
  return(merged_data)
  }

#FUNCTION: rename column names (SMOTE seems to have issues with spaces and other special chars)
rename_columns <- function(dataframe, replacements) {
  for (replacement in replacements) {
    for (col in names(dataframe)) {
      new_col <- gsub(replacement$old, replacement$new, col)
      names(dataframe)[names(dataframe) == col] <- new_col
    }
  }
  return(dataframe)
}

# FUNCTION: apply SMOTE and set output as.factor to allow for randomforest

apply_smote <- function(dataframe, target_col){
  set.seed(1)
  #target_col_data <- dataframe[,target_col]
  
  smote_output <- SMOTE(dataframe[, -which(names(dataframe) == target_col)],
                      dataframe[target_col])
                     
  smote_data <- smote_output$data
  smote_data[[target_col]] <- as.factor(smote_data$class)
  smote_data <- smote_data[, -which(names(smote_data) == "class")]
  #smote_data <- cbind(smote_data, target_col_data)
  return (smote_data)
}

```


### Classification algorithms used

```{r}
# create empty dataframe to store results

# Specify column names
col_names <- c("RF", "LogRes", "SVM","LDA")

# Specify row names (index)
row_names <- c("accuracy","balanced.accuracy","recall","precision","specificity","f1.score" )

# Create an empty data frame with specified column and row names
overall.results.df <- data.frame(matrix(NA, nrow = length(row_names), ncol = length(col_names)))
rownames(overall.results.df) <- row_names
colnames(overall.results.df) <- col_names

# Display the empty data frame
#print(overall.results.df)

#how do assign values: row col
# overall.results.df['accuracy','random_forest'] <- 0.85
```

**1. Random forest:**<br> 
Random forest is an ensemble method which combines multiple models (decision trees) to create predictions. The individual decision trees in random forest takes a random subset of features and the output of these individual decision trees are averaged to create an overall prediction which has low variance. Random forest is known for its robustness against overfitting, ability to provide information on feature importance, and high accuracy in both classification and regression tasks.

```{r, results='hold'}
#PRE-PROCESSING

# 1. Apply minmax and one hot encode to data
processed_data <- preprocess_data(application_clean,target_col='debt_quality')

# 2. Rename columns since it triggers error on SMOTE
replacements <- list(
  list(old = " ", new = "_"),
  list(old = "/", new = "_"),
  list(old = "-", new = "_")
)

processed_data <- rename_columns(processed_data,replacements)

#3. Apply train test split, createDataPartition automatically stratifies classes
set.seed(1)
inTrain <- createDataPartition(processed_data[['debt_quality']],p=0.8)[[1]]
processed_data.train.full <- processed_data[inTrain,]
processed_data.test <- processed_data[-inTrain,]
actual.test <- processed_data.test$debt_quality

set.seed(1)
inTrain <- createDataPartition(processed_data.train.full[['debt_quality']],p=0.8)[[1]]
processed_data.train <- processed_data.train.full[inTrain,]
processed_data.val <- processed_data.train.full[-inTrain,]
actual.val <- processed_data.val$debt_quality

#apply SMOTE to training data only
smote_processed_data <- apply_smote(processed_data.train,target_col='debt_quality')
```

```{r}
# Model and grid search

rf.results.df <- data.frame(ntree = integer(), 
                         mtry= integer(), 
                         accuracy = numeric(),
                         balanced.accuracy = numeric(),
                         recall = numeric(), 
                         precision = numeric(), 
                         specificity = numeric(),
                         f1.score = numeric(),
                         true.positive = integer(),
                         true.negative = integer(),
                         false.positive = integer(),
                         false.negative = integer()
                         )
ntree_vals <- c(900,1100,1300) #c(500,700,900,1100,1300)
mtry_vals <- c(6,7,8) #c(6,7,8) 
for (ntree_val in ntree_vals){
  for (mtry_val in mtry_vals){
    #model
    rf.model <- randomForest(debt_quality~., data = smote_processed_data, ntree=ntree_val, mtry=mtry_val)
    validation.pred <- predict(rf.model, processed_data.val[, -which(names(processed_data.val) == 'debt_quality')])
    cm <- confusionMatrix(validation.pred, actual.val)
    
    #results from confusion matrix
    accuracy <- cm$overall['Accuracy']
    recall <- cm$byClass['Sensitivity']
    precision <- cm$byClass['Precision']
    specificity <- cm$byClass['Specificity']
    balanced.accuracy <- (recall + specificity) /2
    f1.score <- 2 * precision * recall / (precision + recall)
    true.positive <- cm$table[2,2] #true 1
    true.negative <- cm$table[1,1] #true 0
    false.positive <- cm$table[1,2]
    false.negative <- cm$table[2,1]
    
    #add result to dataframe
    rf.results.df <- rbind(rf.results.df,data.frame(ntree = ntree_val, mtry = mtry_val,
                                              accuracy = accuracy,
                                              balanced.accuracy = balanced.accuracy,
                                              recall = recall,
                                              precision = precision,
                                              specificity = specificity,
                                              f1.score = f1.score,
                                              true.positive = true.positive,
                                              true.negative = true.negative,
                                              false.positive = false.positive,
                                              false.negative = false.negative))
  }
}

#remove row names
rownames(rf.results.df) <- NULL
rf.results.df

#plot gridsearch results
ggplot(rf.results.df, aes(x = ntree, y = balanced.accuracy, color = factor(mtry))) +
  geom_line() +
  labs(x = "ntree", y = "balanced accuracy", color = "mtry") +
  scale_color_discrete(name = "mtry") +
  ggtitle('balanced accuracy') +
  theme_minimal()

# Find index of max balanced accuracy and get values of the associated ntree and mtry
max_row_index <- which.max(rf.results.df$balanced.accuracy)
ntree_max <- rf.results.df$ntree[max_row_index]
mtry_max <- rf.results.df$mtry[max_row_index]

# Print the values
cat("Maximum balanced accuracy:", rf.results.df$balanced.accuracy[max_row_index], "\n")
cat("ntree corresponding to maximum balanced accuracy:", ntree_max, "\n")
cat("mtry corresponding to maximum balanced accuracy:", mtry_max, "\n")

```


```{r}
#random forest test score
set.seed(1)

#model
rf.best.model <- randomForest(debt_quality ~., data = processed_data.test, ntree=ntree_max, mtry = mtry_max)
test.pred <- predict (rf.best.model, processed_data.test[, -which(names(processed_data.test) == 'debt_quality')])
cm.best <- confusionMatrix(test.pred, actual.test)
#cm.best

#plot feature importance
varImpPlot(rf.best.model)
#importance(rf.best.model)

# store results of rf best model in overall.results.df
overall.results.df['accuracy','RF'] <- cm.best$overall['Accuracy']
overall.results.df['recall','RF'] <- cm.best$byClass['Sensitivity']
overall.results.df['precision','RF'] <- cm.best$byClass['Precision']
overall.results.df['specificity','RF'] <- cm.best$byClass['Specificity']
overall.results.df['balanced.accuracy','RF'] <- (cm.best$byClass['Sensitivity'] + cm.best$byClass['Specificity'])/2
overall.results.df['f1.score','RF'] <- 2*(cm.best$byClass['Precision'] * cm.best$byClass['Sensitivity'])/((cm.best$byClass['Precision'] + cm.best$byClass['Sensitivity']))
```

2. Logistic regression

```{r}
# Processing variables, create dummy variables, separate categorical variables and numerical variables.
target_data <- application_clean %>% select( 'debt_quality' )
tmpdata <- application_clean %>% select(- 'debt_quality' ) 

class_data <- tmpdata %>% select_if(is.character)
formula <- dummyVars(~ ., data = class_data)
encoded_data <- predict(formula, newdata = class_data)

num_data <- tmpdata %>% select_if(is.numeric)

merged_data <- cbind(num_data, encoded_data,target_data)

merged_data[['debt_quality']] <- as.factor(merged_data[['debt_quality']])

# Imbalanced dataset, hoping for balance.
data_label1 = subset(merged_data, merged_data$debt_quality == 1)
data_label0 = subset(merged_data, merged_data$debt_quality == 0)

#To calculate the number of observations in two subsets.

count_label1 <- nrow(data_label1)
count_label0 <- nrow(data_label0)

# Select the label with fewer observations and then randomly sample from that dataset to make it as many as the more populous label.
set.seed(2)
if (count_label1 < count_label0) {
  data_label0 <- data_label0[sample(nrow(data_label0), count_label1), ]
} else {
  data_label1 <- data_label1[sample(nrow(data_label1), count_label0), ]
}

# Merge datasets
balanced_data <- rbind(data_label0, data_label1)


# Split the dataset into training set and test set

train_index = sample(1:nrow(balanced_data), 0.8 * nrow(balanced_data))
train_data = balanced_data[train_index, ]
test_data = balanced_data[-train_index, ]

# Perform logistic regression analysis.
model = glm(debt_quality  ~ ., family = binomial(),data = train_data )

# Training set accuracy
glm.probs = predict(model,type='response')
glm.pred = rep(0,length(glm.probs))
glm.pred[glm.probs>0.5] = 1

# Calculate confusion matrix and evaluation metrics
confusion_matrix = confusionMatrix(as.factor(glm.pred), train_data$debt_quality)

# Extract evaluation metrics
LogRes.train.table= confusion_matrix$table
LogRes.train.accuracy = confusion_matrix$overall["Accuracy"]
LogRes.train.recall = confusion_matrix$byClass["Recall"]
LogRes.train.precision = confusion_matrix$byClass["Precision"]
LogRes.train.f1 = confusion_matrix$byClass["F1"]

# Print out training results
LogRes.train.table
cat('The training set accuracy is: ',LogRes.train.accuracy,'\n')
cat('The training set recall is: ',LogRes.train.recall,'\n')
cat('The training set precision is: ',LogRes.train.precision,'\n')
cat('The training set f1 is: ',LogRes.train.f1,'\n')

# Testing set accuracy
predictions = predict(model, newdata = test_data,type = "response")
glm.pred2 = rep(0,length(predictions))
glm.pred2[predictions>0.5] = 1

# Calculate confusion matrix and evaluation metrics
confusion_matrix2 = confusionMatrix(as.factor(glm.pred2), test_data$debt_quality)

# Extract evaluation metrics

LogRes.results.table = confusion_matrix2$table
LogRes.results.accuracy = confusion_matrix2$overall["Accuracy"]
LogRes.results.recall = confusion_matrix2$byClass["Recall"]
LogRes.results.precision = confusion_matrix2$byClass["Precision"]
LogRes.results.f1 = confusion_matrix2$byClass["F1"]

# Print out training results
LogRes.results.table
cat('The testing set accuracy is: ',LogRes.results.accuracy,'\n')
cat('The testing set recall is: ',LogRes.results.recall,'\n')
cat('The testing set precision is: ',LogRes.results.precision,'\n')
cat('The testing set f1 is: ',LogRes.results.f1,'\n')

```

3. Support vector machine

```{r}

```


4. linear discriminant analysis
```{r,warning=FALSE, results='hold'}

set.seed(14)

# Train the LDA model, no need for pre-processing as we use SMOTE data from random forest (#1) above
lda_model <- train(debt_quality ~ ., data = smote_processed_data, method = "lda")

# Make predictions using the model
predictions <- predict(lda_model, newdata = processed_data.val)

# Calculate the metrics
conf_mat <- confusionMatrix(predictions, processed_data.val$debt_quality)
acc <- conf_mat$overall['Accuracy']
f1 <- 2 * conf_mat$byClass['Sensitivity'] * conf_mat$byClass['Specificity'] / 
  (conf_mat$byClass['Sensitivity'] + conf_mat$byClass['Specificity'])
recall <- conf_mat$byClass['Sensitivity']
precision <- conf_mat$byClass['Pos Pred Value']


# Print the results
#print(conf_mat)
#print(paste("Accuracy: ", acc))
#print(paste("F1 Score: ", f1))
#print(paste("Recall: ", recall))
#print(paste("Precision: ", precision))

overall.results.df['accuracy','LDA'] <- acc
overall.results.df['recall','LDA'] <- recall
overall.results.df['precision','LDA'] <- precision
overall.results.df['specificity','LDA'] <- conf_mat$byClass['Specificity']
overall.results.df['balanced.accuracy','LDA'] <- (recall+conf_mat$byClass['Specificity'])/2
overall.results.df['f1.score','LDA'] <- f1
```
```

### Classification performance evaluation

[Not final, see planned table below]

```{r}
overall.results.df
```

### Conclusion

[To be done later]
[show which features are most important, if possible]
[summarise if accurately predicting bad debtor is doable or not using application data]

```{r}

```

### References

[.]

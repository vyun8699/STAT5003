---
title: "STAT5003_14_Credit_Card_v1"
author: "STAT5003 group project 14"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
#libraries
library(readr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(scales)
library(GGally)
library(caret)
library(smotefamily)
library(randomForest)
library(reshape2)
library(MLmetrics)
library(ggrepel)
library(scales)
```

### Problem overview

The goal of this exercise is to define 'bad debtors' based on their credit history and identify if a pattern or signal exists amongst credit applicants which may suggest the tendency of a certain cohort to become bad debtors. Management of bad debt is a key value driver in banking / the credit industry. Insights from this analysis may assist banks in acquiring the right customers (e.g. spend marketing budget on potential customers who are less likely to default) and decide on their credit risk management strategy to minimize loss (e.g. allow for different starting debt ceiling for different customer classes, sell off riskier customer portfolio and retain safer ones, etc). The dataset can be found [here](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction).
 
### Dataset description

```{r, message=FALSE, warning=FALSE, results='hold'}
# load dataset(s)
# source: https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction

#get and set path to current working directory
path = getwd()
setwd(path)

#read csv
application_record <- read.csv('application_record.csv', header = TRUE)
credit_record <- read.csv('credit_record.csv', header = TRUE)

# rename features for code simplicity
application_record <- application_record %>%
  rename (gender = CODE_GENDER,
          car_flag = FLAG_OWN_CAR,
          realty_flag = FLAG_OWN_REALTY,
          children_count= CNT_CHILDREN,
          income_total = AMT_INCOME_TOTAL,
          income_type = NAME_INCOME_TYPE,
          education_type = NAME_EDUCATION_TYPE,
          family_status = NAME_FAMILY_STATUS,
          housing_type = NAME_HOUSING_TYPE,
          days_age = DAYS_BIRTH,
          days_employed = DAYS_EMPLOYED,
          mobilephone_flag=FLAG_MOBIL,
          workphone_flag = FLAG_WORK_PHONE,
          phone_flag = FLAG_PHONE,
          email_flag = FLAG_EMAIL,
          occupation_type = OCCUPATION_TYPE,
          familymembers_count = CNT_FAM_MEMBERS
          )

credit_record <- credit_record %>%
  rename (months_balance = MONTHS_BALANCE,
          credit_status = STATUS
          )

#print dimensions
cat("Dimensions of application_record: Rows =", dim(application_record)[1], "Columns =", dim(application_record)[2], "\n")
cat("Dimensions of credit_record: Rows =", dim(credit_record)[1], "Columns =", dim(credit_record)[2], "\n")
```
The dataset is split into two files (**credit_record** and **application_record**) which are connected by the feature **ID**: 

+ **application_record (18 features x 438k observations):**<br> 
  This file contains collected information at the initial credit card application, such as ID, gender, occupation, number of children, marital status, etc.

+ **credit_record (3 features x 1+ million observations):** <br>
  This file contains 46 thousand unique customers IDs which tracks their credit status overtime. Customers are classified into different loan status groups (e.g. 'paid off that month', '1-29 days past due', etc.).

Key challenges that needs to be solved to analyze this dataset include: 

+ **Large and complex dataset**:<br>
  The dataset contains more than 10,000 samples, a mix of numerical and categorical values, and will need to be combined for analysis. **credit_record** will need to be 'flattened' and matched with customer IDs that exists in  **application_record**. The resulting dataset must then be screened for customers with ample transaction history (e.g. we can only learn of 'bad debt' behavior when customer has existed for some time). Post pruning, we are left with a combined dataset of 20 features and 20k+ observations.
  
+ **Class imbalance**:<br>
  The data is imbalanced. The number of 'bad_debtors' is small compared to the overall sample size and will need to be processed to remove bias. Data imbalance is taken into account when defining target classes (e.g. if class size is too small then predicted output will be spurious) and choosing analysis techniques (e.g. choice of performance measure, down-sampling, up-sampling, SMOTE).

### Initial data analysis and cleaning

**1. Screening the dataset for nulls, blanks, and duplicates:**<br>
No nulls were found. Cells in **occupation_type** were blank (" ") for income_type = "pensioners" and were filled in accordingly. Remaining blank cells were filled with "unknown". Rows with duplicated ID numbers in application_data were dropped.

```{r, warning=FALSE, results='hold'}
# check and remove NAs --> no NAs found
na_rows_application <- application_record[!complete.cases(application_record), ]
na_rows_credit <- credit_record[!complete.cases(credit_record), ]
nrow_app <- nrow(na_rows_application) # no rows with NA
nrow_cred <- nrow(na_rows_credit) # no rows with NA

cat('Number of rows with null values in application_record:',nrow_app,'rows','\n')
cat('Number of rows with null values in credit_record:',nrow_cred,'rows','\n')

#OCCUPATION_TYPE has blank values (''), fill with 'Pensioner' and 'Unknown' accordingly

pensioner_count <- 0
unknown_count <- 0

for (i in 1:length(application_record$occupation_type)) {
  if (application_record$occupation_type[i] == "" && application_record$income_type[i] == "Pensioner") {
    application_record$occupation_type[i] <- "Pensioner"
    pensioner_count <- pensioner_count + 1
  }
}

for (i in 1:length(application_record$occupation_type)) {
  if (application_record$occupation_type[i] == "" ) {
    application_record$occupation_type[i] <- "Unknown"
    unknown_count <- unknown_count + 1
  }
}

cat("Number of blank occupation_type filled with 'Pensioner' (inferred from 'income_type'):", pensioner_count,"cells", "\n")
cat("Number of blank occupation_type filled with 'Unknown':", unknown_count,"cells", "\n")

# check and remove duplicates in application_record 
duplicate_rows <- application_record %>% group_by(ID) %>%filter(n() > 1)  
nrow_dup <- nrow(duplicate_rows) 
application_record <- application_record %>% anti_join(duplicate_rows, by = "ID")

cat("Number of duplicate IDs dropped:", nrow_dup,"rows", "\n")
```
**2. Trimming the dataset to only include relevant IDs:**<br>
Only accounts with 20+ month of transaction history were included in the final dataset to allow enough time to pass for each customers to prove their long-term credit quality.

```{r, results='hold'}
# Trim both dataset by common IDs
common_ids <- intersect(application_record$ID, credit_record$ID)
filtered_application_records <- application_record %>% filter(ID %in% common_ids) #36k x 18 fetures
filtered_credit_records <- credit_record %>% filter(ID %in% common_ids) #777k x 3 features

# We can only judge credit quality if customer has already banked with us for some time
# setting a cut off for customers that has banked with us for more than 'threshold' months

threshold <- -20 #this is a negative number since 0 = now

filtered_credit_records_months <- filtered_credit_records  %>% filter(months_balance <= threshold)
length_ids <- unique(filtered_credit_records_months$ID)

cat("There are", length(length_ids),"unique IDs with more than",-threshold,"months of transaction history.","\n")

# trim both datasets to to only include those that meets the threshold history
length_filtered_application_records <- filtered_application_records %>% 
                                          filter(ID %in% length_ids)
length_filtered_credit_records <- filtered_credit_records %>% 
                                          filter(ID %in% length_ids) 

#dimension of filtered records

cat("Dimension of trimmed application_record:", dim(length_filtered_application_records)[1], "rows x",dim(length_filtered_application_records)[2],"features.","\n")

cat("Dimension of trimmed credit_record:", dim(length_filtered_credit_records)[1], "rows x",dim(length_filtered_credit_records)[2],"features.","\n")
```

**3. Understanding the distribution of classes:** <br>
The charts below show the delinquency rate for each bad debt category - inclusive of categories after it (e.g., those with worse credit performance). For example, debtors classified as ‘0-29 days past due’ is inclusive of other debtors with 30+ days past due all the way to write offs. From this chart, we decided to focus on 30+ days delinquency as our definition of ‘bad debt’ since ‘0-29 days’ is too common and could very well be an honest mistake by the customer instead of inability to service debt.

```{r, warning = FALSE, results='hold'}
# currently, STATUS is filled with the following values:
    # X : No loan for the month
    # C : paid off for the month
    # 0 : 0-29 days past due
    # 1 : 30-59 days past due
    # 2 : 60-89 days past due
    # 3 : 90-119 days past due
    # 4 : 120-149 days past due
    # 5 : bad debt / write off

# we want to see the proportion of customers that falls into the '0' to '5' categories
filters <- list(
  c('0','1','2','3','4','5'),
  c('1','2','3','4','5'),
  c('2','3','4','5'),
  c('3','4','5'),
  c('4','5'),
  c('5')
)

category_counts <- numeric(length(filters))
category_ids <- vector("list", length(filters))

for (i in seq_along(filters)) {
  filtered_records <- length_filtered_credit_records %>% filter(credit_status %in% filters[[i]])
  category_ids[[i]] <- unique(filtered_records$ID)
  category_counts[i] <- length(category_ids[[i]])
}

df <- data.frame(
  debt_category = c('0: 0-29 days past due',
                    '1: 30-59 days past due',
                    '2: 60-89 days past due',
                    '3: 90-119 days past due',
                    '4: 120-149 days past due',
                    '5: bad debt / write off'),
  category_count = category_counts
)

df$proportion <- df$category_count / length(length_ids)

# print(df) #unhash to run

bar_plot <- ggplot(df, aes(x = debt_category)) +
  geom_bar(aes(y = category_count), stat = 'identity', fill = 'blue', alpha = 0.5) +
  labs(title = 'Bad debtors (count)', x = 'Category', y = 'Value') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
line_plot <- ggplot(df, aes(x = debt_category)) +
  geom_line(aes(y = proportion, group = 1), color = 'red', size = 1) +
  geom_text(aes(y = proportion, label = percent(proportion, accuracy = 0.1)), vjust = -0.5, color = 'red') +
  labs(title = 'Bad debtors (% total)', x = 'Category', y = 'Value') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(bar_plot, line_plot, ncol = 2)

```

**4. Extracting 'relationship_length' as an additional feature and  'debt_quality' as the target:** <br>

The team identified and added in the length of customer relationship as a potentialy useful feature. The team also created ‘debt_quality’ qualifiers and appended it to **application_record**. Value of ‘0’ for non-problematic debtors, ‘1’ for debtors with 30+ days past due. The team opted to define classes this way as higher classes (e.g. those with longer past dues) have very low counts which can further exacerbate the data imbalance or produce spurious relationships. 

```{r, message=FALSE, warning=FALSE, results='hold'}
# start a new copy of the trimmed application dataset 
application_clean <- length_filtered_application_records

# add relationship_length into dataset
relationship_length_values <- credit_record %>% group_by(ID) %>%
  summarize(relationship_length = -min(months_balance) +1)

application_clean <- application_clean %>% left_join(relationship_length_values, by = "ID")

# add column debt_quality
application_clean$debt_quality <- 0

# depending on how hard we want to make this, we can choose how many flags we want, below we choose 2:
application_clean$debt_quality[application_clean$ID %in% category_ids[[2]]] <- 1
#application_clean$debt_quality[application_clean$ID %in% category_ids[[3]]] <- 2
#application_clean$debt_quality[application_clean$ID %in% category_ids[[4]]] <- 3
#application_clean$debt_quality[application_clean$ID %in% category_ids[[5]]] <- 4
#application_clean$debt_quality[application_clean$ID %in% category_ids[[6]]] <- 5

# Create a table with 'debt_quality' and 'count'
debt_quality_counts <- table(application_clean$debt_quality)
debt_quality_table <- data.frame(debt_quality = names(debt_quality_counts), count = as.vector(debt_quality_counts))

# Print the table
debt_quality_table
```

**5. Data visualization:** <br>

The charts below show composition of bad debt (by count) and income level of each occupation type. We note that some occupancy types carry higher bad debt rates and outliers.
  
```{r, results ='hold', warning = FALSE, fig.show='hold', width= "50%" }
# Count bad debt by occupation type, descending order
debt_count <- application_clean %>% 
  filter(debt_quality >0) %>% 
  group_by(occupation_type) %>% 
  summarise(debt_count=n()) %>% 
  arrange(desc(debt_count))

occupation_count <- application_clean %>%  
  group_by(occupation_type) %>% 
  summarise(occupation_count=n()) 

result <- left_join(occupation_count, debt_count, by = "occupation_type") %>%
  mutate(ratio = debt_count / occupation_count) %>%  arrange(desc(ratio))

result$occupation_type <- factor(result$occupation_type, levels = result$occupation_type)

line_plot <- ggplot(result, aes(x = occupation_type)) +
  geom_line(aes(y = ratio, group = 1), color = 'red', size = 1) +
  geom_text_repel(aes(y = ratio, label = percent(ratio, accuracy = 0.1)), nudge_x = 0.0, nudge_y = 0.05, color = 'red') +
  labs(title = 'Bad debtors by occupation (% total count)', x = 'Category', y = 'Value') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Boxplot, x-axis matches line plot
box_plot <- ggplot(application_clean, aes(x = factor(occupation_type, levels = levels(result$occupation_type)), y = income_total)) + 
  geom_boxplot() +
  labs(title = 'Boxplot of Income by Occupation Type') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "mn")) 

#print them out
print(line_plot)
print(box_plot)
```

### Feature Engineering:

+ 'familymembers_count' was dropped due to high correlation with 'children_count'. 
+ 'income_total' transformed into 'income_total_log' to address long tail. 
+ 'mobilephone_flag' was dropped as it only had a single value. 
+ 'days_age' and 'days_employed' transformed into 'age_years' and 'employment_years'. 
+ 'children_count' was cleaned for outliers (2 rows removed, negligible impact). 

```{r, warning = FALSE, results='hold', fig.show='hold'}
# Plot 1: correlation before cleaning
application_data_numerical <- application_clean %>% select_if(is.numeric)
application_data_numerical$ID <- NULL

correlation_matrix <- cor(application_data_numerical)

corr_1_plot <- ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = 'Correlation before clean-up')+
  coord_fixed(ratio = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#add column income_total_log due to long tail of income_total
application_clean$income_total_log <- log(application_clean$income_total)

# Plot 3 and 4: income_total and income_total_log
bin_count <- 20

p_income_total <-ggplot(application_clean, aes(x = income_total)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'income_total') +
  scale_x_continuous(labels = scales::unit_format(scale = 1e-6, suffix = "mn"))

p_income_total_log<-ggplot(application_clean, aes(x = income_total_log)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'income_total_log') 

#transform days_age into age_years which is now a positive integer
application_clean$age_years = as.integer(round(-application_clean$days_age / 365))

#transform days_employed into employment_years which is now a positive integer, set minimum to 0
application_clean$employment_years = as.integer(round(-application_clean$days_employed / 365))
application_clean$employment_years[application_clean$employment_years < 0] <- 0 

# Plot 5 and 6: days_employed and employment_years
p_days_employed <-ggplot(application_clean, aes(x = days_employed)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'days employed') +
  scale_x_continuous(labels = scales::unit_format(scale = 1e-3, suffix = "k"))

p_employment_years<-ggplot(application_clean, aes(x = employment_years)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'years employed') 


#drop familymembers_count as it highly correlates with children_count + spouse (spouse if married)
application_clean$familymembers_count <- NULL

#drop income_total
application_clean$income_total <- NULL

#mobilephone_flag only has 1 value = 1, drop
application_clean$mobilephone_flag <- NULL

# drop days_age
application_clean$days_age <- NULL

# drop days_employed
application_clean$days_employed <- NULL

# drop ID
application_clean$ID <- NULL

# Plot 7 and 8: children_count before and after outliers removal
p_children_1 <- ggplot(application_data_numerical, aes(x = children_count)) +
    geom_histogram(bins = bin_count) +
    labs(title = 'Children_count (with outliers)')

# remove outliers in children_count
application_clean <- subset(application_clean, children_count <= 10) # 2 rows removed
application_data_numerical <- application_clean %>% select_if(is.numeric)
application_data_numerical$ID <- NULL

p_children_2 <- ggplot(application_data_numerical, aes(x = children_count)) +
        geom_histogram(bins = bin_count) +
        labs(title = 'Children_count (outliers removed)')



#Plot 2: correlation after cleaning
correlation_matrix <- cor(application_data_numerical)

corr_2_plot <- ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = 'Correlation after clean-up')+
  coord_fixed(ratio = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#feature engineering plots
grid.arrange(p_income_total,
             p_income_total_log,
             p_days_employed,
             p_employment_years,
             p_children_1,
             p_children_2, 
             ncol = 2)
```

**Correlation between features improved post clean up, see below:**

```{r}
layout_matrix <- rbind(c(1, 2))
grid.arrange(corr_1_plot, 
             corr_2_plot,
             layout_matrix = layout_matrix)
```

### Pre-processing

**Pre-processing used for this analysis are described below:**<br>

+ **Min-max scaling:** applied to numerical data (excluding the target) to remove bias from large values. 
+ **One-hot-encode:** applied to categorical data (excluding the target) to create binary representation of each categorical value. This is done as some machine learning models require numerical input. 
+ **Synthetic Minority Over-sampling Technique (SMOTE):** applied to prevent bias towards majority class due to data imbalance. SMOTE works by creating synthetic data points between randomly-chosen k-nearest neighbor at random distances in the feature space. In practice, SMOTE should only be used on <mark> training data</mark> to prevent leakage. 

```{r}
# PRE PROCESSING FUNCTIONS

# FUNCTION: minmax scaler, to be used on numerical data in the below function

minmax_scale <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# FUNCTION: apply minmax to numerical data and one hot encode to categorical data.

preprocess_data <- function(dataframe, target_col) {
  #split dataframe into 3 parts for processing
  target_data <- dataframe %>% select({{ target_col }})
  dataframe <- dataframe %>% select(-{{ target_col }}) #take target_data out so it's not accidentally picked
  numerical_data <- dataframe %>% select_if(is.numeric)
  categorical_data <- dataframe %>% select_if(is.character)
  
  #minmax on numerical_data
  numerical_cols <- names(numerical_data)
  scaled_numerical_data <- numerical_data
  
  for (col_name in numerical_cols) {
    scaled_col <- minmax_scale(numerical_data[[col_name]])
    scaled_numerical_data[[col_name]] <- scaled_col
  }
  
  #one hot encode on categorical_data
  formula <- dummyVars(~ ., data = categorical_data)
  encoded_data <- predict(formula, newdata = categorical_data)
  
  #transform target_col as.factor, needed for random forest
  target_data[[target_col]] <- as.factor(target_data[[target_col]])
  
  #bind the three dataframes together
  merged_data <- cbind(scaled_numerical_data, encoded_data, target_data)
  
  return(merged_data)
  }

#FUNCTION: rename column names (SMOTE seems to have issues with spaces and other special chars)
rename_columns <- function(dataframe, replacements) {
  for (replacement in replacements) {
    for (col in names(dataframe)) {
      new_col <- gsub(replacement$old, replacement$new, col)
      names(dataframe)[names(dataframe) == col] <- new_col
    }
  }
  return(dataframe)
}

# FUNCTION: apply SMOTE and set output as.factor to allow for randomforest

apply_smote <- function(dataframe, target_col){
  set.seed(1)
  #target_col_data <- dataframe[,target_col]
  
  smote_output <- SMOTE(dataframe[, -which(names(dataframe) == target_col)],
                      dataframe[target_col])
                     
  smote_data <- smote_output$data
  smote_data[[target_col]] <- as.factor(smote_data$class)
  smote_data <- smote_data[, -which(names(smote_data) == "class")]
  #smote_data <- cbind(smote_data, target_col_data)
  return (smote_data)
}

```


### Classification algorithms used

```{r}
# create empty dataframe to store results

# Specify column names
col_names <- c("RF", "LogRes", "SVM","LDA")

# Specify row names (index)
row_names <- c("accuracy","precision","sensitivity","specificity","balanced.accuracy","f1.score" )

# Create an empty data frame with specified column and row names
overall.results.df <- data.frame(matrix(NA, nrow = length(row_names), ncol = length(col_names)))
rownames(overall.results.df) <- row_names
colnames(overall.results.df) <- col_names


#how do assign values: row col
# overall.results.df['accuracy','random_forest'] <- 0.85
```

**1. Random forest:**<br> 
Random forest is an ensemble method which combines multiple decision trees to create predictions. Random forest is known for its robustness against overfitting as it takes random features in each of the individual decision trees and takes an average/vote of these trees for a combined result. Score charts and parameters for best performance shown below:

```{r, message=FALSE, warning=FALSE, results='hold'}
#PRE-PROCESSING

# 1. Apply minmax and one hot encode to data
processed_data <- preprocess_data(application_clean,target_col='debt_quality')

# 2. Rename columns since it triggers error on SMOTE
replacements <- list(
  list(old = " ", new = "_"),
  list(old = "/", new = "_"),
  list(old = "-", new = "_")
)

processed_data <- rename_columns(processed_data,replacements)

#3. Apply train test split, createDataPartition automatically stratifies classes
set.seed(1)
inTrain <- createDataPartition(processed_data[['debt_quality']],p=0.8)[[1]]
processed_data.train.full <- processed_data[inTrain,]
processed_data.test <- processed_data[-inTrain,]
actual.test <- processed_data.test$debt_quality

set.seed(1)
inTrain <- createDataPartition(processed_data.train.full[['debt_quality']],p=0.8)[[1]]
processed_data.train <- processed_data.train.full[inTrain,]
processed_data.val <- processed_data.train.full[-inTrain,]
actual.val <- processed_data.val$debt_quality

#apply SMOTE to training data only
smote_processed_data <- apply_smote(processed_data.train,target_col='debt_quality')
```

```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
# Model and grid search

#initiate results grid
rf.results.df <- data.frame(ntree = integer(), 
                         mtry= integer(), 
                         accuracy = numeric(),
                         precision = numeric(),
                         sensitivity = numeric(),
                         specificity = numeric(),
                         balanced.accuracy = numeric(),
                         f1.score = numeric(),
                         true.positive = integer(),
                         true.negative = integer(),
                         false.positive = integer(),
                         false.negative = integer()
                         )

#hyperparameter grid
ntree_vals <- c(500,700,900,1100,1300) #c(500,700,900,1100,1300)
mtry_vals <- c(6,7,8) #c(6,7,8) 

#grid search loop
for (ntree_val in ntree_vals){
  for (mtry_val in mtry_vals){
    #model and val
    rf.model <- randomForest(debt_quality~., data = smote_processed_data, ntree=ntree_val, mtry=mtry_val)
    rf.val <- predict(rf.model, processed_data.val[, -which(names(processed_data.val) == 'debt_quality')])
    rf.cm <- confusionMatrix(rf.val, actual.val)
    
    #add result to dataframe
    rf.results.df <- rbind(rf.results.df,data.frame(ntree = ntree_val, mtry = mtry_val,
                                              accuracy = rf.cm$overall['Accuracy'],
                                              precision = rf.cm$byClass['Precision'],
                                              sensitivity = rf.cm$byClass['Sensitivity'],
                                              specificity = rf.cm$byClass['Specificity'],
                                              balanced.accuracy = (rf.cm$byClass['Sensitivity'] + rf.cm$byClass['Specificity']) /2,
                                              f1.score = 2 * rf.cm$byClass['Precision'] * rf.cm$byClass['Sensitivity'] / (rf.cm$byClass['Precision'] + rf.cm$byClass['Sensitivity']),
                                              true.positive = rf.cm$table[2,2],
                                              true.negative = rf.cm$table[1,1],
                                              false.positive = rf.cm$table[1,2],
                                              false.negative = rf.cm$table[2,1]))
  }
}

#remove row names
rownames(rf.results.df) <- NULL

#rf.results.df

#plot gridsearch results
rf.precision.plot <- ggplot(rf.results.df, aes(x = ntree, y = precision, color = factor(mtry))) +
  geom_line() +
  labs(x = "ntree", y = "precision", color = "mtry") +
  scale_color_discrete(name = "mtry") +
  ggtitle('Precision') +
  theme_minimal()

rf.sensitivity.plot <- ggplot(rf.results.df, aes(x = ntree, y = sensitivity, color = factor(mtry))) +
  geom_line() +
  labs(x = "ntree", y = "sensitivity", color = "mtry") +
  scale_color_discrete(name = "mtry") +
  ggtitle('Sensitivity') +
  theme_minimal()

grid.arrange(rf.precision.plot, rf.sensitivity.plot, ncol = 2)

# Find index of max sensitivity and get values of the associated ntree and mtry
max_row_index <- which.max(rf.results.df$sensitivity)
ntree_max <- rf.results.df$ntree[max_row_index]
mtry_max <- rf.results.df$mtry[max_row_index]

# Print the values
cat("Maximum sensitivity:", rf.results.df$sensitivity[max_row_index], "\n")
cat("ntree corresponding to maximum balanced accuracy:", ntree_max, "\n")
cat("mtry corresponding to maximum balanced accuracy:", mtry_max, "\n")

```

Top-10 most important features (by Gini impurity)  shown below. We can see that 'relationship_length', 'age_years', 'income_total_log', and 'employment_years' are the key drivers to prediction outcome. 

```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
#random forest test score
set.seed(1)

#model
rf.best.model <- randomForest(debt_quality ~., data = processed_data.test, ntree=ntree_max, mtry = mtry_max)
rf.test.pred <- predict (rf.best.model, processed_data.test[, -which(names(processed_data.test) == 'debt_quality')])
rf.cm.best <- confusionMatrix(rf.test.pred, actual.test)

#plot top-10 feature importance
varImpPlot(rf.best.model, n.var=10, main ='Top-10 most important features')

# store results of rf best model in overall.results.df
overall.results.df['accuracy','RF'] <- rf.cm.best$overall['Accuracy']
overall.results.df['precision','RF'] <- rf.cm.best$byClass['Precision']
overall.results.df['sensitivity','RF'] <- rf.cm.best$byClass['Sensitivity']
overall.results.df['specificity','RF'] <- rf.cm.best$byClass['Specificity']
overall.results.df['balanced.accuracy','RF'] <- (rf.cm.best$byClass['Sensitivity'] + rf.cm.best$byClass['Specificity'])/2
overall.results.df['f1.score','RF'] <- 2*(rf.cm.best$byClass['Precision'] * rf.cm.best$byClass['Sensitivity'])/((rf.cm.best$byClass['Precision'] + rf.cm.best$byClass['Sensitivity']))
```

**2. Logistic regression:**<br>


```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
# Processing variables, create dummy variables, separate categorical variables and numerical variables.
target_data <- application_clean %>% select( 'debt_quality' )
tmpdata <- application_clean %>% select(- 'debt_quality' ) 

class_data <- tmpdata %>% select_if(is.character)
formula <- dummyVars(~ ., data = class_data)
encoded_data <- predict(formula, newdata = class_data)

num_data <- tmpdata %>% select_if(is.numeric)

merged_data <- cbind(num_data, encoded_data,target_data)

merged_data[['debt_quality']] <- as.factor(merged_data[['debt_quality']])

# Imbalanced dataset, hoping for balance.
data_label1 = subset(merged_data, merged_data$debt_quality == 1)
data_label0 = subset(merged_data, merged_data$debt_quality == 0)

#To calculate the number of observations in two subsets.

count_label1 <- nrow(data_label1)
count_label0 <- nrow(data_label0)

# Select the label with fewer observations and then randomly sample from that dataset to make it as many as the more populous label.
set.seed(2)
if (count_label1 < count_label0) {
  data_label0 <- data_label0[sample(nrow(data_label0), count_label1), ]
} else {
  data_label1 <- data_label1[sample(nrow(data_label1), count_label0), ]
}

# Merge datasets
balanced_data <- rbind(data_label0, data_label1)

# Split the dataset into training set and test set
train_index = sample(1:nrow(balanced_data), 0.8 * nrow(balanced_data))
train_data = balanced_data[train_index, ]
test_data = balanced_data[-train_index, ]

# Perform logistic regression analysis.
model = glm(debt_quality  ~ ., family = binomial(),data = train_data )

# Training set accuracy
glm.probs = predict(model,type='response')
glm.pred = rep(0,length(glm.probs))
glm.pred[glm.probs>0.5] = 1

# Calculate confusion matrix and evaluation metrics
confusion_matrix = confusionMatrix(as.factor(glm.pred), train_data$debt_quality)

# Extract evaluation metrics
LogRes.train.table= confusion_matrix$table
LogRes.train.accuracy = confusion_matrix$overall["Accuracy"]
LogRes.train.precision = confusion_matrix$byClass["Precision"]
LogRes.train.sensitivity = confusion_matrix$byClass["Sensitivity"]
LogRes.train.specificity = confusion_matrix$byClass["Specificity"]
LogRes.train.balanced.accuracy = (LogRes.train.sensitivity + LogRes.train.specificity)/2
LogRes.train.f1 = confusion_matrix$byClass["F1"]

# Print out training results
LogRes.train.table
cat('The training set accuracy is: ',LogRes.train.accuracy,'\n')
cat('The training set precision is: ',LogRes.train.precision,'\n')
cat('The training set sensitivity is: ',LogRes.train.sensitivity,'\n')
cat('The training set specificity is: ',LogRes.train.specificity,'\n')
cat('The training set balanced accuracy is: ',LogRes.train.balanced.accuracy,'\n')
cat('The training set f1 is: ',LogRes.train.f1,'\n')

# Testing set accuracy
predictions = predict(model, newdata = test_data,type = "response")
glm.pred2 = rep(0,length(predictions))
glm.pred2[predictions>0.5] = 1

# Calculate confusion matrix and evaluation metrics
confusion_matrix2 = confusionMatrix(as.factor(glm.pred2), test_data$debt_quality)

# Extract evaluation metrics

LogRes.results.table = confusion_matrix2$table
LogRes.results.accuracy = confusion_matrix2$overall["Accuracy"]
LogRes.results.precision = confusion_matrix2$byClass["Precision"]
LogRes.results.sensitivity = confusion_matrix2$byClass["Sensitivity"]
LogRes.results.specificity = confusion_matrix2$byClass["Specificity"]
LogRes.results.balanced.accuracy = (LogRes.results.sensitivity + LogRes.results.specificity)/2
LogRes.results.f1 = confusion_matrix2$byClass["F1"]

# Print out training results
LogRes.results.table
cat('The testing set accuracy is: ',LogRes.results.accuracy,'\n')
cat('The testing set precision is: ',LogRes.results.precision,'\n')
cat('The testing set sensitivity is: ',LogRes.results.sensitivity,'\n')
cat('The testing set specificity is: ',LogRes.results.specificity,'\n')
cat('The testing set balance accuracy is: ',LogRes.results.balanced.accuracy,'\n')
cat('The testing set f1 is: ',LogRes.results.f1,'\n')

# store results of logres in overall.results.df
overall.results.df['accuracy','LogRes'] <- LogRes.results.accuracy
overall.results.df['precision','LogRes'] <- LogRes.results.precision
overall.results.df['sensitivity','LogRes'] <- LogRes.results.sensitivity
overall.results.df['specificity','LogRes'] <- LogRes.results.specificity
overall.results.df['balanced.accuracy','LogRes'] <- LogRes.results.balanced.accuracy
overall.results.df['f1.score','LogRes'] <- LogRes.results.f1
```



**3. linear discriminant analysis:** <br>
LDA is a dimensionality reduction technique that works by finding the linear combination of features that best separates multiple classes in data.  It is very interpretable, and very robust against overfitting since it best estimates the discriminant boundaries using covariance information. The key thing to note about this method is that unlike other techniques, atleast in R, it does not require any hyperparameter tuning, since the  lda() function in the MASS package tunes parameters itself, using the covariance matrices of the input data. This means that for this technique, only cross validation has been done to ensure robustness. All the preprocessing necessary for LDA has already been done under Random Forest, so the same data will be used.  
```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}

set.seed(14)

# Set up the train control
ctrl <- trainControl(method = "cv", number = 5)

# Perform LDA with cross-validation
lda_model <- train(debt_quality ~ ., 
                   data = smote_processed_data, 
                   method = "lda",
                   trControl = ctrl)

# Access the accuracy values from each iteration of the cross-validation
cv_results <- data.frame(Accuracy = lda_model$resample$Accuracy, Iteration = 1:5)

# Plot the change in accuracy for each run of the cross-validation
ggplot(cv_results, aes(x = Iteration, y = Accuracy)) +
  geom_line() +
  geom_point() +
  labs(title = "Change in Accuracy for each Cross-Validation Run", x = "Iteration", y = "Accuracy") +
  theme_minimal()

# Print the results
print(lda_model)

# Extract the best model
best_lda <- lda_model$finalModel

# Use the best model to make predictions on the test set
test.pred <- predict(best_lda, newdata = processed_data.test[, -which(names(processed_data.test) == 'debt_quality')])
cm_best <- confusionMatrix(test.pred$class, actual.test)

# Store results of the best model in overall.results.df
overall.results.df['accuracy', 'LDA'] <- cm_best$overall['Accuracy']
overall.results.df['sensitivity', 'LDA'] <- cm_best$byClass['Sensitivity']
overall.results.df['precision', 'LDA'] <- cm_best$byClass['Precision']
overall.results.df['specificity', 'LDA'] <- cm_best$byClass['Specificity']
overall.results.df['balanced.accuracy', 'LDA'] <- (cm_best$byClass['Sensitivity'] + cm_best$byClass['Specificity']) / 2
```
As you can see , the best model has been chosen after cross validating over five runs, to ensure that the best and most robust model is chosen. 

**4. Support vector machine:** <br>
SVM is a powerful supervised algorithm that can perform well in handling complex datasets with non-linear decision boundaries by utilizing kernel functions to transform the data into a high-dimensional space.


```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
#create train and test data

processed_data <- preprocess_data(application_clean, target_col='debt_quality')

#apply train test split, createDataPartition automatically stratifies classes
set.seed(5003)
inTrain <- createDataPartition(processed_data[['debt_quality']],p=0.8)[[1]]
processed_data.train <- processed_data[inTrain,]
processed_data.test <- processed_data[-inTrain,]

```
```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
#libraries
library(e1071)
library(MASS)
library(mlr)

# Apply grid search for SVM
grid_search_svm <- function(trainset) {
  col_names <- colnames(trainset)
  fixed_col_names <- make.names(col_names, unique = TRUE)
  colnames(trainset) <- fixed_col_names
  task <- makeClassifTask(data = trainset, target = "debt_quality")

  # creat params
  ps <- makeParamSet(
    makeDiscreteParam("cost", values = c(0.01, 0.1, 1)),
    makeDiscreteParam("gamma", values = c(0.001, 0.01, 0.1))
  )

  # create a makelearner
  learner <- makeLearner("classif.svm", predict.type = "prob")

  # run grid search
  res <- tuneParams(learner, task, resampling = cv3, par.set = ps, control = makeTuneControlGrid())

  # take a long time to search
  return(res$x)
}


```

```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
#Utilize feature selection techniques

#Use pca for selecting features
select_data <- preProcess(processed_data.train, method = "pca", pcaComp = 10)
train_data_pca <- predict(select_data, processed_data.train)
test_data_pca <- predict(select_data, processed_data.test)
#Use grid search to tune params
tuned_params <- grid_search_svm(train_data_pca)
pca.svm.model <- svm(debt_quality ~ ., data = train_data_pca, kernel = "radial", cost = tuned_params[0], gamma = tuned_params[1])

```

```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}

# Predict on the test data set using pca
pca.svm.predictions <- predict(pca.svm.model, test_data_pca)
confusion_matrix_pca <- confusionMatrix(pca.svm.predictions, reference = test_data_pca$debt_quality)

# Print out training results
confusion_matrix_pca$table
cat('The testing set accuracy is: ',confusion_matrix_pca$overall["Accuracy"],'\n')
cat("Precision:",  confusion_matrix_pca$byClass["Precision"], "\n")
cat("Recall:",  confusion_matrix_pca$byClass["Recall"], "\n")
cat("F1 Score:", 2 * (confusion_matrix_pca$byClass["Precision"] * confusion_matrix_pca$byClass["Recall"]) / (confusion_matrix_pca$byClass["Precision"] + confusion_matrix_pca$byClass["Recall"]), "\n")

overall.results.df['accuracy', 'SVM'] <- confusion_matrix_pca$overall["Accuracy"]
overall.results.df['recall', 'SVM'] <- confusion_matrix_pca$byClass["Sensitivity"]
overall.results.df['precision', 'SVM'] <- confusion_matrix_pca$byClass["Precision"]
overall.results.df['specificity', 'SVM'] <- confusion_matrix_pca$byClass["Specificity"]
overall.results.df['balanced.accuracy', 'SVM'] <- (confusion_matrix_pca$byClass["Sensitivity"] + confusion_matrix_pca$byClass["Specificity"]) / 2

```


### Classification performance evaluation

We measure model performance through **precision** (TP / (TP+FP)) e.g., out of all customers predicted as bad debtors, how many are really bad debtors? and **sensitivity** (TP/(TP+FN)) e.g., out of all actual bad debtors, how many were actually flagged as bad debtors by the model?. Both of these measures are important as we want to (i) maximize the model's ability to predict true positives, and (ii) minimize intances of false positive and false negative. As displayed below, RF significantly perform better on both metrics compared to others:

```{r, message=FALSE, warning=FALSE, results='hold', fig.show='hold'}
overall.results.df
```

### Conclusion

We recommend Random Forest (RF) for this analysis, though it still has its limitations, as shown by its specificity score (0.58, caused by moderately high false positives). This implies that the RF model is 'too strict' : where some good debtors are also flagged as bad debtors. This limitation may be caused by data imbalance which has been minimized by SMOTE but still presents some bias. We are comfortable with this flaw as having false positives carry lesser economic impact than having false negatives (e.g., lost revenue instead of bad debt on the banking book).

To improve model performance, additional data may be needed (e.g. higher volume of customer data and expanding features collected at application). Other useful features that may be collected include information on wealth level, history of indebtedness at other financial institutions, criminal records,  etc. If we had a greater amount of resources, perhaps a deep learning model could have been trained on a significantl large enough dataset to ensure better results, and also ensure that it 'learns' from the data such that it can handle outliers on its own and be used in multiple use cases and more extensively. 


### References

[.]

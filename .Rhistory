filter(ID %in% length_ids)
#dimension of filtered records
cat("Dimension of trimmed application_record:", dim(length_filtered_application_records)[1], "rows x",dim(length_filtered_application_records)[2],"features.","\n")
cat("Dimension of trimmed credit_record:", dim(length_filtered_credit_records)[1], "rows x",dim(length_filtered_credit_records)[2],"features.","\n")
# currently, STATUS is filled with the following values:
# X : No loan for the month
# C : paid off for the month
# 0 : 0-29 days past due
# 1 : 30-59 days past due
# 2 : 60-89 days past due
# 3 : 90-119 days past due
# 4 : 120-149 days past due
# 5 : bad debt / write off
# we want to see the proportion of customers that falls into the '0' to '5' categories
filters <- list(
c('0','1','2','3','4','5'),
c('1','2','3','4','5'),
c('2','3','4','5'),
c('3','4','5'),
c('4','5'),
c('5')
)
category_counts <- numeric(length(filters))
category_ids <- vector("list", length(filters))
for (i in seq_along(filters)) {
filtered_records <- length_filtered_credit_records %>% filter(credit_status %in% filters[[i]])
category_ids[[i]] <- unique(filtered_records$ID)
category_counts[i] <- length(category_ids[[i]])
}
df <- data.frame(
debt_category = c('0: 0-29 days past due',
'1: 30-59 days past due',
'2: 60-89 days past due',
'3: 90-119 days past due',
'4: 120-149 days past due',
'5: bad debt / write off'),
category_count = category_counts
)
df$proportion <- df$category_count / length(length_ids)
# print(df) #unhash to run
bar_plot <- ggplot(df, aes(x = debt_category)) +
geom_bar(aes(y = category_count), stat = 'identity', fill = 'blue', alpha = 0.5) +
labs(title = 'Bad debtors (count)', x = 'Category', y = 'Value') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
line_plot <- ggplot(df, aes(x = debt_category)) +
geom_line(aes(y = proportion, group = 1), color = 'red', size = 1) +
geom_text(aes(y = proportion, label = percent(proportion, accuracy = 0.1)), vjust = -0.5, color = 'red') +
labs(title = 'Bad debtors (% total)', x = 'Category', y = 'Value') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
grid.arrange(bar_plot, line_plot, ncol = 2)
# start a new copy of the trimmed application dataset
application_clean <- length_filtered_application_records
# add relationship_length into dataset
relationship_length_values <- credit_record %>% group_by(ID) %>%
summarize(relationship_length = -min(months_balance) +1)
application_clean <- application_clean %>% left_join(relationship_length_values, by = "ID")
# add column debt_quality
application_clean$debt_quality <- 0
# depending on how hard we want to make this, we can choose how many flags we want, below we choose 2:
application_clean$debt_quality[application_clean$ID %in% category_ids[[2]]] <- 1
#application_clean$debt_quality[application_clean$ID %in% category_ids[[3]]] <- 2
#application_clean$debt_quality[application_clean$ID %in% category_ids[[4]]] <- 3
#application_clean$debt_quality[application_clean$ID %in% category_ids[[5]]] <- 4
#application_clean$debt_quality[application_clean$ID %in% category_ids[[6]]] <- 5
# Create a table with 'debt_quality' and 'count'
debt_quality_counts <- table(application_clean$debt_quality)
debt_quality_table <- data.frame(debt_quality = names(debt_quality_counts), count = as.vector(debt_quality_counts))
# Print the table
debt_quality_table
# Count bad debt by occupation type, descending order
debt_count <- application_clean %>%
filter(debt_quality >0) %>%
group_by(occupation_type) %>%
summarise(debt_count=n()) %>%
arrange(desc(debt_count))
occupation_count <- application_clean %>%
group_by(occupation_type) %>%
summarise(occupation_count=n())
result <- left_join(occupation_count, debt_count, by = "occupation_type") %>%
mutate(ratio = debt_count / occupation_count) %>%  arrange(desc(ratio))
result$occupation_type <- factor(result$occupation_type, levels = result$occupation_type)
line_plot <- ggplot(result, aes(x = occupation_type)) +
geom_line(aes(y = ratio, group = 1), color = 'red', size = 1) +
geom_text_repel(aes(y = ratio, label = percent(ratio, accuracy = 0.1)), nudge_x = 0.0, nudge_y = 0.05, color = 'red') +
labs(title = 'Bad debtors by occupation (% total count)', x = 'Category', y = 'Value') +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#Boxplot, x-axis matches line plot
box_plot <- ggplot(application_clean, aes(x = factor(occupation_type, levels = levels(result$occupation_type)), y = income_total)) +
geom_boxplot() +
labs(title = 'Boxplot of Income by Occupation Type') +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "mn"))
#print them out
print(line_plot)
print(box_plot)
# Plot 1: correlation before cleaning
application_data_numerical <- application_clean %>% select_if(is.numeric)
application_data_numerical$ID <- NULL
correlation_matrix <- cor(application_data_numerical)
corr_1_plot <- ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "blue", high = "red") +
theme_minimal() +
labs(title = 'Correlation before clean-up')+
coord_fixed(ratio = 1) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#add column income_total_log due to long tail of income_total
application_clean$income_total_log <- log(application_clean$income_total)
# Plot 3 and 4: income_total and income_total_log
bin_count <- 20
p_income_total <-ggplot(application_clean, aes(x = income_total)) +
geom_histogram(bins = bin_count) +
labs(title = 'income_total') +
scale_x_continuous(labels = scales::unit_format(scale = 1e-6, suffix = "mn"))
p_income_total_log<-ggplot(application_clean, aes(x = income_total_log)) +
geom_histogram(bins = bin_count) +
labs(title = 'income_total_log')
#transform days_age into age_years which is now a positive integer
application_clean$age_years = as.integer(round(-application_clean$days_age / 365))
#transform days_employed into employment_years which is now a positive integer, set minimum to 0
application_clean$employment_years = as.integer(round(-application_clean$days_employed / 365))
application_clean$employment_years[application_clean$employment_years < 0] <- 0
# Plot 5 and 6: days_employed and employment_years
p_days_employed <-ggplot(application_clean, aes(x = days_employed)) +
geom_histogram(bins = bin_count) +
labs(title = 'days employed') +
scale_x_continuous(labels = scales::unit_format(scale = 1e-3, suffix = "k"))
p_employment_years<-ggplot(application_clean, aes(x = employment_years)) +
geom_histogram(bins = bin_count) +
labs(title = 'years employed')
#drop familymembers_count as it highly correlates with children_count + spouse (spouse if married)
application_clean$familymembers_count <- NULL
#drop income_total
application_clean$income_total <- NULL
#mobilephone_flag only has 1 value = 1, drop
application_clean$mobilephone_flag <- NULL
# drop days_age
application_clean$days_age <- NULL
# drop days_employed
application_clean$days_employed <- NULL
# drop ID
application_clean$ID <- NULL
# Plot 7 and 8: children_count before and after outliers removal
p_children_1 <- ggplot(application_data_numerical, aes(x = children_count)) +
geom_histogram(bins = bin_count) +
labs(title = 'Children_count (with outliers)')
# remove outliers in children_count
application_clean <- subset(application_clean, children_count <= 10) # 2 rows removed
application_data_numerical <- application_clean %>% select_if(is.numeric)
application_data_numerical$ID <- NULL
p_children_2 <- ggplot(application_data_numerical, aes(x = children_count)) +
geom_histogram(bins = bin_count) +
labs(title = 'Children_count (outliers removed)')
#Plot 2: correlation after cleaning
correlation_matrix <- cor(application_data_numerical)
corr_2_plot <- ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "blue", high = "red") +
theme_minimal() +
labs(title = 'Correlation after clean-up')+
coord_fixed(ratio = 1) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#feature engineering plots
grid.arrange(p_income_total,
p_income_total_log,
p_days_employed,
p_employment_years,
p_children_1,
p_children_2,
ncol = 2)
layout_matrix <- rbind(c(1, 2))
grid.arrange(corr_1_plot,
corr_2_plot,
layout_matrix = layout_matrix)
# PRE PROCESSING FUNCTIONS
# FUNCTION: minmax scaler, to be used on numerical data in the below function
minmax_scale <- function(x) {
(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
# FUNCTION: apply minmax to numerical data and one hot encode to categorical data.
preprocess_data <- function(dataframe, target_col) {
#split dataframe into 3 parts for processing
target_data <- dataframe %>% select({{ target_col }})
dataframe <- dataframe %>% select(-{{ target_col }}) #take target_data out so it's not accidentally picked
numerical_data <- dataframe %>% select_if(is.numeric)
categorical_data <- dataframe %>% select_if(is.character)
#minmax on numerical_data
numerical_cols <- names(numerical_data)
scaled_numerical_data <- numerical_data
for (col_name in numerical_cols) {
scaled_col <- minmax_scale(numerical_data[[col_name]])
scaled_numerical_data[[col_name]] <- scaled_col
}
#one hot encode on categorical_data
formula <- dummyVars(~ ., data = categorical_data)
encoded_data <- predict(formula, newdata = categorical_data)
#transform target_col as.factor, needed for random forest
target_data[[target_col]] <- as.factor(target_data[[target_col]])
#bind the three dataframes together
merged_data <- cbind(scaled_numerical_data, encoded_data, target_data)
return(merged_data)
}
#FUNCTION: rename column names (SMOTE seems to have issues with spaces and other special chars)
rename_columns <- function(dataframe, replacements) {
for (replacement in replacements) {
for (col in names(dataframe)) {
new_col <- gsub(replacement$old, replacement$new, col)
names(dataframe)[names(dataframe) == col] <- new_col
}
}
return(dataframe)
}
# FUNCTION: apply SMOTE and set output as.factor to allow for randomforest
apply_smote <- function(dataframe, target_col){
set.seed(1)
#target_col_data <- dataframe[,target_col]
smote_output <- SMOTE(dataframe[, -which(names(dataframe) == target_col)],
dataframe[target_col])
smote_data <- smote_output$data
smote_data[[target_col]] <- as.factor(smote_data$class)
smote_data <- smote_data[, -which(names(smote_data) == "class")]
#smote_data <- cbind(smote_data, target_col_data)
return (smote_data)
}
# create empty dataframe to store results
# Specify column names
col_names <- c("RF", "LogRes", "SVM","LDA")
# Specify row names (index)
row_names <- c("accuracy","precision","sensitivity","specificity","balanced.accuracy","f1.score" )
# Create an empty data frame with specified column and row names
overall.results.df <- data.frame(matrix(NA, nrow = length(row_names), ncol = length(col_names)))
rownames(overall.results.df) <- row_names
colnames(overall.results.df) <- col_names
#how do assign values: row col
# overall.results.df['accuracy','random_forest'] <- 0.85
#PRE-PROCESSING
# 1. Apply minmax and one hot encode to data
processed_data <- preprocess_data(application_clean,target_col='debt_quality')
# 2. Rename columns since it triggers error on SMOTE
replacements <- list(
list(old = " ", new = "_"),
list(old = "/", new = "_"),
list(old = "-", new = "_")
)
processed_data <- rename_columns(processed_data,replacements)
#3. Apply train test split, createDataPartition automatically stratifies classes
set.seed(1)
inTrain <- createDataPartition(processed_data[['debt_quality']],p=0.8)[[1]]
processed_data.train.full <- processed_data[inTrain,]
processed_data.test <- processed_data[-inTrain,]
actual.test <- processed_data.test$debt_quality
set.seed(1)
inTrain <- createDataPartition(processed_data.train.full[['debt_quality']],p=0.8)[[1]]
processed_data.train <- processed_data.train.full[inTrain,]
processed_data.val <- processed_data.train.full[-inTrain,]
actual.val <- processed_data.val$debt_quality
#apply SMOTE to training data only
smote_processed_data <- apply_smote(processed_data.train,target_col='debt_quality')
# Model and grid search
#initiate results grid
rf.results.df <- data.frame(ntree = integer(),
mtry= integer(),
accuracy = numeric(),
precision = numeric(),
sensitivity = numeric(),
specificity = numeric(),
balanced.accuracy = numeric(),
f1.score = numeric(),
true.positive = integer(),
true.negative = integer(),
false.positive = integer(),
false.negative = integer()
)
#hyperparameter grid
ntree_vals <- c(500,700,900,1100,1300) #c(500,700,900,1100,1300)
mtry_vals <- c(6,7,8) #c(6,7,8)
#grid search loop
for (ntree_val in ntree_vals){
for (mtry_val in mtry_vals){
#model and val
rf.model <- randomForest(debt_quality~., data = smote_processed_data, ntree=ntree_val, mtry=mtry_val)
rf.val <- predict(rf.model, processed_data.val[, -which(names(processed_data.val) == 'debt_quality')])
rf.cm <- confusionMatrix(rf.val, actual.val)
#add result to dataframe
rf.results.df <- rbind(rf.results.df,data.frame(ntree = ntree_val, mtry = mtry_val,
accuracy = rf.cm$overall['Accuracy'],
precision = rf.cm$byClass['Precision'],
sensitivity = rf.cm$byClass['Sensitivity'],
specificity = rf.cm$byClass['Specificity'],
balanced.accuracy = (rf.cm$byClass['Sensitivity'] + rf.cm$byClass['Specificity']) /2,
f1.score = 2 * rf.cm$byClass['Precision'] * rf.cm$byClass['Sensitivity'] / (rf.cm$byClass['Precision'] + rf.cm$byClass['Sensitivity']),
true.positive = rf.cm$table[2,2],
true.negative = rf.cm$table[1,1],
false.positive = rf.cm$table[1,2],
false.negative = rf.cm$table[2,1]))
}
}
#remove row names
rownames(rf.results.df) <- NULL
#rf.results.df
#plot gridsearch results
rf.precision.plot <- ggplot(rf.results.df, aes(x = ntree, y = precision, color = factor(mtry))) +
geom_line() +
labs(x = "ntree", y = "precision", color = "mtry") +
scale_color_discrete(name = "mtry") +
ggtitle('Precision') +
theme_minimal()
rf.sensitivity.plot <- ggplot(rf.results.df, aes(x = ntree, y = sensitivity, color = factor(mtry))) +
geom_line() +
labs(x = "ntree", y = "sensitivity", color = "mtry") +
scale_color_discrete(name = "mtry") +
ggtitle('Sensitivity') +
theme_minimal()
grid.arrange(rf.precision.plot, rf.sensitivity.plot, ncol = 2)
# Find index of max sensitivity and get values of the associated ntree and mtry
max_row_index <- which.max(rf.results.df$sensitivity)
ntree_max <- rf.results.df$ntree[max_row_index]
mtry_max <- rf.results.df$mtry[max_row_index]
# Print the values
cat("Maximum sensitivity:", rf.results.df$sensitivity[max_row_index], "\n")
cat("ntree corresponding to maximum balanced accuracy:", ntree_max, "\n")
cat("mtry corresponding to maximum balanced accuracy:", mtry_max, "\n")
#random forest test score
set.seed(1)
#model
rf.best.model <- randomForest(debt_quality ~., data = processed_data.test, ntree=ntree_max, mtry = mtry_max)
rf.test.pred <- predict (rf.best.model, processed_data.test[, -which(names(processed_data.test) == 'debt_quality')])
rf.cm.best <- confusionMatrix(rf.test.pred, actual.test)
#plot top-10 feature importance
varImpPlot(rf.best.model, n.var=10, main ='Top-10 most important features')
# store results of rf best model in overall.results.df
overall.results.df['accuracy','RF'] <- rf.cm.best$overall['Accuracy']
overall.results.df['precision','RF'] <- rf.cm.best$byClass['Precision']
overall.results.df['sensitivity','RF'] <- rf.cm.best$byClass['Sensitivity']
overall.results.df['specificity','RF'] <- rf.cm.best$byClass['Specificity']
overall.results.df['balanced.accuracy','RF'] <- (rf.cm.best$byClass['Sensitivity'] + rf.cm.best$byClass['Specificity'])/2
overall.results.df['f1.score','RF'] <- 2*(rf.cm.best$byClass['Precision'] * rf.cm.best$byClass['Sensitivity'])/((rf.cm.best$byClass['Precision'] + rf.cm.best$byClass['Sensitivity']))
# Processing variables, create dummy variables, separate categorical variables and numerical variables.
target_data <- application_clean %>% select( 'debt_quality' )
tmpdata <- application_clean %>% select(- 'debt_quality' )
class_data <- tmpdata %>% select_if(is.character)
formula <- dummyVars(~ ., data = class_data)
encoded_data <- predict(formula, newdata = class_data)
num_data <- tmpdata %>% select_if(is.numeric)
merged_data <- cbind(num_data, encoded_data,target_data)
merged_data[['debt_quality']] <- as.factor(merged_data[['debt_quality']])
# Imbalanced dataset, hoping for balance.
data_label1 = subset(merged_data, merged_data$debt_quality == 1)
data_label0 = subset(merged_data, merged_data$debt_quality == 0)
#To calculate the number of observations in two subsets.
count_label1 <- nrow(data_label1)
count_label0 <- nrow(data_label0)
# Select the label with fewer observations and then randomly sample from that dataset to make it as many as the more populous label.
set.seed(2)
if (count_label1 < count_label0) {
data_label0 <- data_label0[sample(nrow(data_label0), count_label1), ]
} else {
data_label1 <- data_label1[sample(nrow(data_label1), count_label0), ]
}
# Merge datasets
balanced_data <- rbind(data_label0, data_label1)
# Split the dataset into training set and test set
train_index = sample(1:nrow(balanced_data), 0.8 * nrow(balanced_data))
train_data = balanced_data[train_index, ]
test_data = balanced_data[-train_index, ]
# Perform logistic regression analysis.
model = glm(debt_quality  ~ ., family = binomial(),data = train_data )
# Training set accuracy
glm.probs = predict(model,type='response')
glm.pred = rep(0,length(glm.probs))
glm.pred[glm.probs>0.5] = 1
# Calculate confusion matrix and evaluation metrics
confusion_matrix = confusionMatrix(as.factor(glm.pred), train_data$debt_quality)
# Extract evaluation metrics
LogRes.train.table= confusion_matrix$table
LogRes.train.accuracy = confusion_matrix$overall["Accuracy"]
LogRes.train.precision = confusion_matrix$byClass["Precision"]
LogRes.train.sensitivity = confusion_matrix$byClass["Sensitivity"]
LogRes.train.specificity = confusion_matrix$byClass["Specificity"]
LogRes.train.balanced.accuracy = (LogRes.train.sensitivity + LogRes.train.specificity)/2
LogRes.train.f1 = confusion_matrix$byClass["F1"]
# Print out training results
LogRes.train.table
cat('The training set accuracy is: ',LogRes.train.accuracy,'\n')
cat('The training set precision is: ',LogRes.train.precision,'\n')
cat('The training set sensitivity is: ',LogRes.train.sensitivity,'\n')
cat('The training set specificity is: ',LogRes.train.specificity,'\n')
cat('The training set balanced accuracy is: ',LogRes.train.balanced.accuracy,'\n')
cat('The training set f1 is: ',LogRes.train.f1,'\n')
# Testing set accuracy
predictions = predict(model, newdata = test_data,type = "response")
glm.pred2 = rep(0,length(predictions))
glm.pred2[predictions>0.5] = 1
# Calculate confusion matrix and evaluation metrics
confusion_matrix2 = confusionMatrix(as.factor(glm.pred2), test_data$debt_quality)
# Extract evaluation metrics
LogRes.results.table = confusion_matrix2$table
LogRes.results.accuracy = confusion_matrix2$overall["Accuracy"]
LogRes.results.precision = confusion_matrix2$byClass["Precision"]
LogRes.results.sensitivity = confusion_matrix2$byClass["Sensitivity"]
LogRes.results.specificity = confusion_matrix2$byClass["Specificity"]
LogRes.results.balanced.accuracy = (LogRes.results.sensitivity + LogRes.results.specificity)/2
LogRes.results.f1 = confusion_matrix2$byClass["F1"]
# Print out training results
LogRes.results.table
cat('The testing set accuracy is: ',LogRes.results.accuracy,'\n')
cat('The testing set precision is: ',LogRes.results.precision,'\n')
cat('The testing set sensitivity is: ',LogRes.results.sensitivity,'\n')
cat('The testing set specificity is: ',LogRes.results.specificity,'\n')
cat('The testing set balance accuracy is: ',LogRes.results.balanced.accuracy,'\n')
cat('The testing set f1 is: ',LogRes.results.f1,'\n')
# store results of logres in overall.results.df
overall.results.df['accuracy','LogRes'] <- LogRes.results.accuracy
overall.results.df['precision','LogRes'] <- LogRes.results.precision
overall.results.df['sensitivity','LogRes'] <- LogRes.results.sensitivity
overall.results.df['specificity','LogRes'] <- LogRes.results.specificity
overall.results.df['balanced.accuracy','LogRes'] <- LogRes.results.balanced.accuracy
overall.results.df['f1.score','LogRes'] <- LogRes.results.f1
#Pre-processing data
#min-max scale
min_max_scale <- function(df) {
scale(df, center = min(df), scale = max(df) - min(df))
}
#pre-process data
preprocess_data <- function(dataframe, target_col) {
target_data <- dataframe %>% dplyr::select({{ target_col }})
dataframe <- dataframe %>% dplyr::select(-{{ target_col }}) #take target_data out so it's not accidentally picked
numerical_data <- dataframe %>% select_if(is.numeric)
categorical_data <- dataframe %>% select_if(is.character)
#apply min-max scale for the numerical data
numerical_data <- lapply(numerical_data, min_max_scale)
numerical_data_frame <- as.data.frame(numerical_data)
#one hot data encode on the categorical data
formula <- dummyVars(~ ., data = categorical_data)
encoded_data <- predict(formula, newdata = categorical_data)
#transform target_col as.factor, needed for svm
target_data[[target_col]] <- as.factor(target_data[[target_col]])
#bind the three dataframes together
merged_data <- cbind(numerical_data_frame, encoded_data, target_data)
return(merged_data)
}
processed_data <- preprocess_data(application_clean, target_col='debt_quality')
#create train and test data
processed_data <- preprocess_data(application_clean, target_col='debt_quality')
#apply train test split, createDataPartition automatically stratifies classes
set.seed(5003)
inTrain <- createDataPartition(processed_data[['debt_quality']],p=0.8)[[1]]
processed_data.train <- processed_data[inTrain,]
processed_data.test <- processed_data[-inTrain,]
# Apply grid search for SVM
col_names <- colnames(processed_data.train)
fixed_col_names <- make.names(col_names, unique = TRUE)
colnames(processed_data.train) <- fixed_col_names
task <- makeClassifTask(data = processed_data.train, target = "debt_quality")
install.packages('e1071')
install.packages("e1071")
install.packages("MASS")
install.packages("mlr")
knitr::opts_chunk$set(echo = TRUE)
#Pre-processing data
#min-max scale
min_max_scale <- function(df) {
scale(df, center = min(df), scale = max(df) - min(df))
}
#pre-process data
preprocess_data <- function(dataframe, target_col) {
target_data <- dataframe %>% dplyr::select({{ target_col }})
dataframe <- dataframe %>% dplyr::select(-{{ target_col }}) #take target_data out so it's not accidentally picked
numerical_data <- dataframe %>% select_if(is.numeric)
categorical_data <- dataframe %>% select_if(is.character)
#apply min-max scale for the numerical data
numerical_data <- lapply(numerical_data, min_max_scale)
numerical_data_frame <- as.data.frame(numerical_data)
#one hot data encode on the categorical data
formula <- dummyVars(~ ., data = categorical_data)
encoded_data <- predict(formula, newdata = categorical_data)
#transform target_col as.factor, needed for svm
target_data[[target_col]] <- as.factor(target_data[[target_col]])
#bind the three dataframes together
merged_data <- cbind(numerical_data_frame, encoded_data, target_data)
return(merged_data)
}
processed_data <- preprocess_data(application_clean, target_col='debt_quality')
#Pre-processing data
#min-max scale
min_max_scale <- function(df) {
scale(df, center = min(df), scale = max(df) - min(df))
}
#pre-process data
preprocess_data <- function(dataframe, target_col) {
target_data <- dataframe %>% dplyr::select({{ target_col }})
dataframe <- dataframe %>% dplyr::select(-{{ target_col }}) #take target_data out so it's not accidentally picked
numerical_data <- dataframe %>% select_if(is.numeric)
categorical_data <- dataframe %>% select_if(is.character)
#apply min-max scale for the numerical data
numerical_data <- lapply(numerical_data, min_max_scale)
numerical_data_frame <- as.data.frame(numerical_data)
#one hot data encode on the categorical data
formula <- dummyVars(~ ., data = categorical_data)
encoded_data <- predict(formula, newdata = categorical_data)
#transform target_col as.factor, needed for svm
target_data[[target_col]] <- as.factor(target_data[[target_col]])
#bind the three dataframes together
merged_data <- cbind(numerical_data_frame, encoded_data, target_data)
return(merged_data)
}
processed_data <- preprocess_data(application_clean, target_col='debt_quality')
# Apply grid search for SVM
col_names <- colnames(processed_data.train)
fixed_col_names <- make.names(col_names, unique = TRUE)
colnames(processed_data.train) <- fixed_col_names
task <- makeClassifTask(data = processed_data.train, target = "debt_quality")
